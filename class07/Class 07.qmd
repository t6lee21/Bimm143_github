---
title: "class 07: Machine learning 1"
author: "Trinity Lee A16639698"
format: pdf
---

#clustering

We will start today's lab with clustering methods, in particular so-called K-means. The main function for this is in R is `kmeans()`.

Lets try it on some made up data where we know what the answer should be.

```{r}
x<-rnorm(10000,mean=3)
hist(x)

```

60 points 
We can pass the function to `plot()`
```{r}
tmp<-c(rnorm(30,mean=3),rnorm(30,-3))
x<-cbind(x=tmp,y=rev(tmp))
x
head(x)
plot(x)
```

```{r}
k<-kmeans(x,centers=2,nstart=20)
k
```

Q1. How many points are in each cluster?

```{r}
k$size
```

Q2. CLuster membership?

```{r}
k$cluster
```

Q3. CLuster centers?

```{r}
k$centers
```

Q4. plot my clustering results.

```{r}
plot(x, col=k$cluster,pch=16)
```

Q5. cluster the data again with `kmeans()` into 4 groups and plot the results.

```{r}
k4<-kmeans(x,centers=4,nstart=20)
plot(x, col=k4$cluster,pch=16)
```

k-means is popular mostly because it is fast and relatively straightforward to run and understand. It has a big limitation in that you need to tell it how many groups (k, or centers) you want. 

#Hierarchial clustering
the main fucntion in base R is called `hclust()`. You have to pass it in a "distance matrix" nto just your input data.

You can generate a distance matrix with the `dist()` fucntion.

```{r}
hc<-hclust(dist(x))
hc
```

```{r}
plot(hc)
```

To find clusters (cluster membership vector) from a `hclust()` reuslt we can "cut" the tree at a certain height that we like.

```{r}
grps<-cutree(hc,h=8)
plot(x, col=grps,pch=16)
abline(h=8,col="red")
table(grps)
```
#Principal component analysis

##PCA of UK food data
Read data showing the comsumption of food in grams (per person, per week) of 17 different types of food-stuff measured and averaged in the four countries of the United Kingdom in 1997.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x

```
Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
ans<-dim(x)
ans
```
## Preview the first 6 rows
```{r}
head(x)
```
The row names seem to be incorrect with X as the first one. Let's try to fix this and get the correct amount of rows.
```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
ans1<-dim(x)
ans1
```
To fix the issue we have when rerunning the data file again we will use `read.csv()`

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```

Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer using the row.names argument set to read.csv() due to it fixing the issue while allowing us to rerun the data without messing it up. Therefore it is more robust and less prone to error. 

#Spotting major differences and trends
A cursory glance over the numbers in this table does not reveal much of anything. Indeed in general it is difficult to extract meaning in regard to major differences and trends from any given array of numbers. Generating regular bar-plots and various pairwise plots does not help too much either
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

The figure shows graphs of the comparison of the amounts of each food consumed by two countries. If a point lies on the diagonal for a graph it means that exactly the same amount or similar amounts of that food are consumed in the two countires being compared. 
Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

There seems to be two major food categories that are different or deviate from the diagonal between N.IReland and other UK countries but we cannot determine which ones exactly from just looking at the graph. 

##PCA

PCA can help us make sense of these types of datasets. Let's see how it works.

The main function in "base R" is called `prcomp()`. In this case we want to first take the transpose of our input `x` so the columns are the food types and the countries are the rows. 

```{r}
head(t(x))
```
```{r}
pca<-prcomp(t(x))
summary(pca)
```

Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange","red","blue","green"),pch=16,xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```
The "loadings" tell us how much of the original variables -foods contribute to the new variables -PCs

```{r}
head(pca$rotation)
```

we can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
## or the second row here...
z <- summary(pca)
z$importance
```

This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number), which is given below.

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

#Digging deeper 
We can also consider the influence of each of the original variables upon the principal components (typically known as loading scores). This information can be obtained from the `prcomp()` returned `$rotation` component. It can also be summarized with a call to `biplot()`

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
The two food groups are fresh potatoes and soft drinks. PC2 mainly tells us about the left over varience that PC1 does not cover. 
